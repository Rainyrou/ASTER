hydra:
  searchpath:
    - file://verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

data:
  max_prompt_length: 2048
  max_response_length: 16384
  train_batch_size: 256
  return_raw_chat: True
  filter_overlong_prompts: False
  truncation: 'error'
  tool_config_path: ${actor_rollout_ref.multi_turn.tool_config_path}
  enable_thinking: True

actor_rollout_ref:
  hybrid_engine: True
  rollout:
    name: sglang
    multi_turn:
      enable: True
      max_assistant_turns: 50
      tool_config_path: "./config/tool_config/sandbox_fusion_tool_config.yaml"
    enable_thinking: True
  actor:
    use_dynamic_bsz: True
    use_kl_loss: False
    kl_loss_coef: 0.0
    kl_loss_type: low_var_kl
    entropy_coeff: 0
  model:
    use_remove_padding: True
    use_liger: True
    enable_gradient_checkpointing: True

algorithm:
  use_kl_in_reward: False

trainer:
  log_train_generations: 5
  save_hf_ckpts: false
  profiler:
    timeline_logging: true
    output_dir: ${trainer.default_local_dir}/timeline_logs

reward_model:
  reward_manager: aster

ray_kwargs:
  ray_init:
    runtime_env:
      env_vars:
        TORCH_NCCL_AVOID_RECORD_STREAMS: "1"
        CUDA_DEVICE_MAX_CONNECTIONS: "1"
        NCCL_IB_DISABLE: '0'
        NCCL_DEBUG: 'INFO'
